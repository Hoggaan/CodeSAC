{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ac924c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found. Error loading \"C:\\Users\\akafi\\anaconda3\\envs\\aima\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22040/1981648105.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aima\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' Error loading \"{dll}\" or one of its dependencies.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found. Error loading \"C:\\Users\\akafi\\anaconda3\\envs\\aima\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.nn.Functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class replayBuffer:\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        \n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        \n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        reward = self.reward_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "        \n",
    "        return states_, actions, states, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims=256, fc2_dims=256, n_actions, name='critic', chkpt_dir='tmp/sac'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.name = name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name + '_sac')\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dims[0] + n_actions, self.fc1)\n",
    "        self.fc2 = nn.Linear(self.self.fc1_dims, self.fc2_dims)\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state, actions):\n",
    "        action_value = self.fc1(T.cat([state, action], dim = 1))\n",
    "        action_value = F.relu(actions_values)\n",
    "        action_value = self.fc2(action_value)\n",
    "        action_value = F.relu(action_value)\n",
    "        \n",
    "        q = self.q(action_value)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict_dict(T.load(self.checkpoint_file))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims=256, fc2_dims=256, name='value', chkpt_dir='tmp/sac'):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.name = name\n",
    "        self.checkpoint_dir = chkpt_dir \n",
    "        self.checkpoint_files = os.path.join(self.checkpoint_dir, name +'_sac')\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.input_dim, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.v = nn.Linear(self.fc2_dims, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = F.relu(state_value)\n",
    "        \n",
    "        q = self.q(state_value)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict_dict(T.load(self.checkpoint_file))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8986861",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (Temp/ipykernel_21064/2138916224.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\akafi\\AppData\\Local\\Temp/ipykernel_21064/2138916224.py\"\u001b[1;36m, line \u001b[1;32m22\u001b[0m\n\u001b[1;33m    self.device() = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "# Actor Network\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims,  max_action, n_actions, fc1_dims=256, fc2_dims=256,  name='actor', chkpt_dir='tmp/sac'):\n",
    "        super(ActorNetwork, self).__init()\n",
    "        \n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.name = name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_sac')\n",
    "        self.max_action = max_action\n",
    "        self.reparam_noise = 1e-6\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.sigma = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        \n",
    "        self.optimizers = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device() = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        prob = self.fc1(state)\n",
    "        prob = F.relu(prob)\n",
    "        prob = self.fc2(prob)\n",
    "        prob = F.relu(prob)\n",
    "        \n",
    "        mu = self.mu(prob)\n",
    "        sigma = self.sigma(prob)\n",
    "        \n",
    "        sigma = T.clamp(sigma, min=self.reparam_noise, max=1)\n",
    "        \n",
    "        return mu, sigma\n",
    "    def sample_normal(self, state, reparameterize=True):\n",
    "        mu, sigma = self.forward(state)\n",
    "        probabilities = Normal(mu, sigma)\n",
    "        \n",
    "        if reparameterize:\n",
    "            actions = probabilities.rsample()\n",
    "        else:\n",
    "            actions = probabilities.sample\n",
    "            \n",
    "        action = T.tanh(actions) * T.tensor(self.max_action).to(self.device)\n",
    "        log_probs = probabilities.log_prob(actions)\n",
    "        log_probs -= T.log(1 - action.pow(2) + self.reparam_noise)\n",
    "        log_probs = log_probs.sum(1, keepdim=True)\n",
    "        \n",
    "        return action, log_probs\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict_dict(T.load(self.checkpoint_file))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc3b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAC agent\n",
    "class Agent:\n",
    "    def __init__(self, alpha = 0.0003, beta = 0.0003, input_dims=[8], env=None, gamma=0.99, n_actions=2, max_size=1000000, \\\n",
    "                 layer1_size=256, layer2_size=256, batch_size=256, reward_scale=2, tau=0.005):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.actor = ActorNetwork(alpha,input_dims, n_actions = n_actions, name = 'actor', max_action = env.action_space.high)\n",
    "        self.critic1 = CriticNetwork(beta, input_dims, n_actions = n_actions, name='critic_1')\n",
    "        self.critic2 = CriticNetwork(beta, input_dims, n_actions = n_actions, name='critic_2')\n",
    "        self.value = ValueNetwork(beta, input_dims, name = 'value')\n",
    "        self.target_value = ValueNetwork(beta, input_dims, name='target_value')\n",
    "        \n",
    "        self.scale = reward_scale\n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def choos_action(self, observation):\n",
    "        state = T.tensor([observation]).to(self.actor.device)\n",
    "        actions, _ = self.actor.sample_normal(state, reparameterize=False)\n",
    "        \n",
    "        return actions.cpu.detach().numpy()[0]\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, new_state, reward, done)\n",
    "    \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        \n",
    "        target_value_params = self.target_value.named_parameters()\n",
    "        value_params = self.value.named_parameters()\n",
    "        \n",
    "        target_value_state_dict = dict(target_value_params)\n",
    "        value_state_dict = dict(value_params)\n",
    "        \n",
    "        for name in value_state_dict:\n",
    "            value_state_dict[name] = tau * value_state_dict[name].clone() + (1 - tau * target_value_state_dict[name].clone)\n",
    "            \n",
    "        self.target_value.load_state_dict(value_state_dict)\n",
    "        \n",
    "    def save_models(self):\n",
    "        print('..... Saving model ....')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.value.save_checkpoint()\n",
    "        self.target_value.save_checkpoint()\n",
    "        \n",
    "        self.critic_1.save_checkpoint()\n",
    "        self.critic_2.save_checkpoint()\n",
    "        \n",
    "        def load_models(self):\n",
    "            print('..... Loading Models .....')\n",
    "            self.value.load_checkpoint()\n",
    "            self.target_value.load_checkpoint()\n",
    "            self.critic_1.load_checkpoint()\n",
    "            self.critic_2.load_checkpoint()\n",
    "        \n",
    "        def learn():\n",
    "            if self.memory.mem_cntr < self.batch_size:\n",
    "                return \n",
    "            \n",
    "            state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "            \n",
    "            reward = T.tensor(reward, dtype=T.float).to(self.actor.device)\n",
    "            done = T.tensor(done).to(self.actor.device)\n",
    "            state_ = T.tensor(new_state, dtype=T.float).to(self.actor.device)\n",
    "            state = T.tensor(state, dtype=T.float).to(self.actor.device)\n",
    "            action = T.tensor(action, dtype=T.float).to(self.actor.device)\n",
    "            \n",
    "            value = self.value(state).view(-1)\n",
    "            value_ = self.target_value(state_).view(-1)\n",
    "            value_[done] = 0.0\n",
    "            \n",
    "            actions, log_probs = self.actor.sample_normal(state, reparameterize=False)\n",
    "            log_probs = log_probs.view(-1)\n",
    "            q1_new_policy = self.critic_1.forward(state, actions)\n",
    "            q2_new_policy = self.critic_2.forward(state, actions)\n",
    "            critic_value = T.min(q1_new_policy, q2_new_policy)\n",
    "            critic_value = critic_value.view(-1)\n",
    "            \n",
    "            self.value.optimizer.zero_grad()\n",
    "            value_target = critic_value - log_probs\n",
    "            value_los = 0.5 * F.mse_loss(value, value_target)\n",
    "            value_loss.backward(retain_graph=True)\n",
    "            self.value.optimizer.step()\n",
    "            \n",
    "            actions, log_probs = self.actor.sample_normal(state, reparameterize=True)\n",
    "            log_probs = log_probs.view(-1)\n",
    "            q1_new_policy = self.critic_1.forward(state, actions)\n",
    "            q2_new_policy = self.critic_2.forward(state, actions)\n",
    "            critic_value = T.min(q1_new_policy, q2_new_policy)\n",
    "            critic_value = critic_value.view(-1)\n",
    "            \n",
    "            actor_loss = log_probs - critic_value\n",
    "            actor_loss = T.mean(actor_loss)\n",
    "            self.actor.optimizer.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            self.actor.optimizer.step()\n",
    "            \n",
    "            self.critic_1.optimizer.zero_grad()\n",
    "            self.critic_2.optimizer.zero_grad()\n",
    "            q_hat = self.scale*reward + self.self.gamma*value_\n",
    "            q1_old_policy = self.critic_1.forward(state, action).view(-1)\n",
    "            q2_old_policy = self.critic_2.forward(state, action).view(-1)\n",
    "            critic_1_loss = 0.5 * F.mse_loss(q1_old_policy, q_hat)\n",
    "            critic_2_loss = 0.5 * F.mse_loss(q2_old_policy, q_hat)\n",
    "            \n",
    "            critic_loss = critic_1_loss + critic_2_loss\n",
    "            critic_loss.backward()\n",
    "            self.critic_1.optimizer.step()\n",
    "            self.critic_2.optimizer.step()\n",
    "            \n",
    "            self.update_network_parameters()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b54b01",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21064/2200610425.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import pybullet_envs\n",
    "import gym\n",
    "import numpy as np\n",
    "from utils import plot_learning_curve\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env.gym.make('InvertedPendulumBulletEnv-v0')\n",
    "    agent = Agent(input_dims=env.observation_space, env = env, n_actions = env.action_space.shape[0])\n",
    "    n_games = 250\n",
    "    file_name = 'inverted_pendulum.png'\n",
    "    \n",
    "    figure_file = 'plots/' + file_name\n",
    "    \n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "    load_checkpoint = False\n",
    "    if load_checkpoint:\n",
    "        agent.load_models()\n",
    "        env.render(mode = 'human')\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0 \n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.remember(observation, action, reward, observation_, done)\n",
    "            if not load_checkpoint:\n",
    "                agent.learn()\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            if not load_checkpoint:\n",
    "                agent.save_models()\n",
    "        \n",
    "        print('episode ', i, 'score %.1f ' % score, 'avg_score %.1f' % avg_score)\n",
    "    if not load_checkpoint:\n",
    "        x = [i + 1 for i in range(n_games)]\n",
    "        plot_learning_curve(x, score_history, figure_file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2580ea9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
